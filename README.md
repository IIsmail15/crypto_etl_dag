# ğŸª™ Crypto ETL Pipeline with Apache Airflow

This project demonstrates a simple Extract-Transform-Load (ETL) pipeline for cryptocurrency market data using **Apache Airflow**, **Python**, and the **CoinGecko API**.

---

## ğŸš€ Features

- Extracts top 10 cryptocurrencies by market cap from CoinGecko
- Transforms JSON data into a clean CSV format
- Stores raw and transformed data in a local folder
- Runs on Airflow with Docker Compose
- Uses LocalExecutor with PostgreSQL as metadata DB

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ dags/ # Airflow DAGs
â”‚ â””â”€â”€ crypto_etl_dag.py # Main ETL DAG
â”œâ”€â”€ data/ # Saved raw + transformed data (mounted volume)
â”œâ”€â”€ plugins/ # (Optional) custom operators/hooks
â”œâ”€â”€ docker-compose.yml # Docker setup for Airflow + Postgres
â”œâ”€â”€ .env.example # Environment variable template
â”œâ”€â”€ .gitignore # Ignores real .env and logs
â””â”€â”€ README.md # This file


```
---

## âš™ï¸ Setup Instructions

### 1. Clone this repository

```bash
git clone https://github.com/IIsmail15/crypto_etl_dag.git
cd crypto_etl_dag
```

 ### 2. Create your .env file
 

```
cp .env.example .env

```

### 3. Create necessary folders


```
mkdir -p dags logs plugins output

```

###  Build and run Airflow


```
docker compose up --build

```
Airflow UI will be available at: http://localhost:8080
Login using the credentials from your .env file.

## ğŸ“… DAG Overview

- **DAG ID**: `crypto_etl_pipeline_etl_steps`
- **Schedule**: Daily (`@daily`)

### Tasks

1. **extract_crypto_data** â€“ Fetches JSON from CoinGecko API  
2. **transform_crypto_data** â€“ Converts to DataFrame and saves CSV  
3. **load_crypto_data** â€“ Prints a confirmation (simulating a load step)

---

## ğŸ“‚ Output Files

After the DAG runs successfully, you should see:

- `data/raw_crypto.json`
- `data/crypto_prices.csv`

---

## ğŸ“œ License

**MIT** â€” free to use, modify, and share.
